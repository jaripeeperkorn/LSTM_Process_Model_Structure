{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFONNTMiXKK-",
    "outputId": "77a6feb9-b796-47d3-e237-79e55374c9d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Bidirectional, LSTM, Activation, Dropout, Embedding, Input\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "\n",
    "import os.path\n",
    "\n",
    "def save_log(loglist, filename): #save a list of lists \n",
    "  df = pd.DataFrame.from_records(loglist)\n",
    "  df.to_csv(filename, index=False)\n",
    "\n",
    "def remove_nan(lists):\n",
    "  newlists = []\n",
    "  for tr in lists:\n",
    "    newlists.append([int(x) for x in tr if str(x) != 'nan'])\n",
    "  return(newlists)\n",
    "\n",
    "def import_log(filepath):\n",
    "  df = pd.read_csv(filepath)\n",
    "  return(remove_nan(df.values.tolist()))\n",
    "\n",
    "\n",
    "\n",
    "def number_to_one_hot_X(X, dict_size): #if we want \n",
    "  newX = []\n",
    "  for example in X:\n",
    "    new_ex = []\n",
    "    for i in range(len(example)):\n",
    "      onehot = [0]*dict_size #changed\n",
    "      if example[i] != 0:\n",
    "        onehot[example[i] - 1] = 1 #-1 because begin counting at 0\n",
    "      new_ex.append(onehot)\n",
    "    newX.append(new_ex)\n",
    "  return(np.array(newX))\n",
    "\n",
    "def create_XY_prefix(log, mappingsize, prefixlen):\n",
    "  X = []\n",
    "  Y = []\n",
    "  for i in range(0, len(log)):\n",
    "    for k in range(1, len(log[i])):\n",
    "      X.append(log[i][max(0, k-prefixlen):k]) #get the prefix of 'encoded' activities\n",
    "      y = [0] *(mappingsize)\n",
    "      y[int(log[i][k])-1] = 1\n",
    "      Y.append(y)        \n",
    "  X = keras.preprocessing.sequence.pad_sequences(X, maxlen=prefixlen, padding='pre')\n",
    "  X = number_to_one_hot_X(X, mappingsize)\n",
    "  return(np.array(X), np.array(Y))\n",
    "\n",
    "def get_startend(log): \n",
    "  return log[0][0], log[0][-1]\n",
    "\n",
    "def get_model(maxlen, num_chars, bidirec, n_layers, lstmsize, dropout, l1, l2):\n",
    "  model = Sequential()\n",
    "  model.add(Input(shape=(maxlen, num_chars))) #If you don't use an embedding layer input should be one-hot-encoded\n",
    "  if bidirec == False:   \n",
    "    model.add(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=(n_layers != 1),kernel_regularizer=regularizers.l1_l2(l1,l2),\n",
    "                   recurrent_regularizer=regularizers.l1_l2(l1,l2),input_shape=(maxlen, num_chars)))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(1, n_layers):\n",
    "      return_sequences = (i+1 != n_layers)\n",
    "      model.add(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=return_sequences,\n",
    "                     kernel_regularizer=regularizers.l1_l2(l1,l2),recurrent_regularizer=regularizers.l1_l2(l1,l2)))\n",
    "      model.add(Dropout(dropout))\n",
    "  else:\n",
    "    model.add(Bidirectional(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=(n_layers != 1),kernel_regularizer=regularizers.l1_l2(l1,l2),\n",
    "                   recurrent_regularizer=regularizers.l1_l2(l1,l2),input_shape=(maxlen, num_chars))))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(1, n_layers):\n",
    "      return_sequences = (i+1 != n_layers)\n",
    "      model.add(Bidirectional(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=return_sequences,\n",
    "                     kernel_regularizer=regularizers.l1_l2(l1,l2),recurrent_regularizer=regularizers.l1_l2(l1,l2))))\n",
    "      model.add(Dropout(dropout))\n",
    "  model.add(Dense(num_chars, kernel_initializer='glorot_uniform',activation='softmax'))\n",
    "  opt = Adam(learning_rate=0.005)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics='accuracy')\n",
    "  return model\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train,batch_size, maxlen, num_chars, bidirec, n_layers, lstmsize, dropout, l1, l2):\n",
    "  model = get_model(maxlen, num_chars, bidirec, n_layers, lstmsize, dropout, l1, l2)\n",
    "  model.summary()\n",
    "  early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "  #train_model\n",
    "  history = model.fit(X_train, y_train, validation_split=0.2, callbacks=[early_stopping, lr_reducer], batch_size=batch_size, epochs=600, verbose=2)\n",
    "  return model\n",
    "\n",
    "def cut_end(log, endact):\n",
    "  logsize, tracesize = log.shape\n",
    "  print(log.shape)\n",
    "  newlog = []\n",
    "  for i in range(0, logsize):\n",
    "    trace = []\n",
    "    for j in range(0, tracesize):\n",
    "      if log[i][j] == endact:\n",
    "        trace.append(log[i][j])\n",
    "        break\n",
    "      else:\n",
    "        trace.append(log[i][j])\n",
    "    newlog.append(trace)\n",
    "  return(newlog)\n",
    "\n",
    "def normalize(probs): #normalize probabilities to sum to 1\n",
    "  examplesize, actsize = probs.shape\n",
    "  newy = []\n",
    "  for i in range(examplesize):\n",
    "    normalizer = 1 / float( sum(probs[i]) )\n",
    "    ynorm = [float(l) * normalizer for l in probs[i]]\n",
    "    newy.append(ynorm)\n",
    "  return newy\n",
    "\n",
    "\n",
    "def choose_act_all(all_y): #randomly choose an activity, stochastically\n",
    "  #p want a list of probabilities    \n",
    "  chosen_acts = []\n",
    "  for i in range(len(all_y)):\n",
    "      chosen_acts.append(np.random.choice(np.arange(0, len(all_y[i])), p=all_y[i])+1)  \n",
    "  return(chosen_acts)   # +1 because number encodig starts at 1 not 0\n",
    "\n",
    "def OHget_probabilities(rnnmodel, xlists,  nr_act, maxlen, prefixlen):\n",
    "  #assume xlist is a list with the x (prefix) untill now \n",
    "  all_x = keras.preprocessing.sequence.pad_sequences(xlists, maxlen=maxlen, padding=\"pre\")\n",
    "  all_x = all_x[:,-(prefixlen):]\n",
    "  all_x = number_to_one_hot_X(all_x, nr_act)\n",
    "  results = rnnmodel.predict(all_x)\n",
    "  return results\n",
    "\n",
    "def OHsimulate_log(RNNmodel, logsize, startact, endact, maxlen, mapping, prefixlen): #Use RNN to simulate log\n",
    "  log = np.zeros((logsize, maxlen+1), int)\n",
    "  for i in range(0, logsize): #start every trace with the start activity\n",
    "    log[i][0] = startact\n",
    "  print(log)\n",
    "  for j in range(1,maxlen+1): #check if 0 or 1 and ml or ml - 1 #we took 50 for with loops   \n",
    "    print(\"finding activity nr\", j+1)   \n",
    "    prefixes = np.array([log[i][0:j] for i in range(0, logsize)])\n",
    "    print(prefixes)\n",
    "    probs = OHget_probabilities(RNNmodel, prefixes, len(mapping), maxlen, prefixlen)\n",
    "    #we need to do this because otherwise probabilities sum over 1 \n",
    "    ynorm = normalize(probs) \n",
    "    nextacts = choose_act_all(ynorm) \n",
    "    for i in range(0, logsize):\n",
    "      log[i][j] = nextacts[i]\n",
    "  print(log)\n",
    "  corrected_log = cut_end(log, endact)      \n",
    "  return(corrected_log) \n",
    "\n",
    "\n",
    "def do_experiment(full_log_location, train_log_location, sim_log_location, map_location, fold, full_prefix, opt_prefixlen, size_sim_log,\n",
    "                 bidirec=True, n_layers=1, lstmsize=64, dropout=0.4, l1=0.001, l2=0.001):\n",
    "  full_log = remove_nan(import_log(full_log_location))\n",
    "\n",
    "  maxlen = len(max(full_log,key=len))\n",
    "  #if we want to use the full prefix each time or not\n",
    "  if full_prefix == True:\n",
    "    prefixlen=maxlen - 1\n",
    "    print(\"prefix length:\", prefixlen)\n",
    "  else:\n",
    "    prefixlen=opt_prefixlen\n",
    "\n",
    "  #reload mapping\n",
    "  mappingfilename = map_location\n",
    "  with open(mappingfilename) as f:\n",
    "    mapping = json.loads(f.read())\n",
    "\n",
    "  batch_size = 128\n",
    "\n",
    "\n",
    "  for i in range(0, fold):\n",
    "    if os.path.exists(SimLogPath):\n",
    "      print(\"Already done: \", i)\n",
    "      continue\n",
    "    train_log = import_log(train_log_location)\n",
    "    start,end = get_startend(train_log)\n",
    "    X_train, y_train = create_XY_prefix(train_log, len(mapping), prefixlen)\n",
    "    model = train_model(X_train, y_train,batch_size, maxlen=prefixlen, num_chars=len(mapping), bidirec=bidirec, n_layers=n_layers, lstmsize=lstmsize, dropout=dropout, l1=l1, l2=l2)\n",
    "\n",
    "    simlog = OHsimulate_log(model, size_sim_log, start, end, maxlen-1, mapping, prefixlen)\n",
    "    \n",
    "    save_log(simlog, sim_log_location)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Fold3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
