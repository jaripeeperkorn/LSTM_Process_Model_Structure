{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnFQJYU5j8SP"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Bidirectional, LSTM, Activation, Dropout, Embedding, Input\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjotXh4vxg_s"
   },
   "outputs": [],
   "source": [
    "def import_log(filepath):\n",
    "  df = pd.read_csv(filepath)\n",
    "  return(df.values.tolist())\n",
    "\n",
    "def remove_nan(lists):\n",
    "  newlists = []\n",
    "  for tr in lists:\n",
    "    newlists.append([int(x) for x in tr if str(x) != 'nan'])\n",
    "  return(newlists)\n",
    "\n",
    "def number_to_one_hot_X(X, dict_size): #if we want \n",
    "  newX = []\n",
    "  for example in X:\n",
    "    new_ex = []\n",
    "    for i in range(len(example)):\n",
    "      onehot = [0]*dict_size #changed\n",
    "      if example[i] != 0:\n",
    "        onehot[example[i] - 1] = 1 #-1 because begin counting at 0\n",
    "      new_ex.append(onehot)\n",
    "    newX.append(new_ex)\n",
    "  return(np.array(newX))\n",
    "\n",
    "def create_XY_prefix(log, mappingsize, prefixlen):\n",
    "  X = []\n",
    "  Y = []\n",
    "  for i in range(0, len(log)):\n",
    "    for k in range(1, len(log[i])):\n",
    "      X.append(log[i][max(0, k-prefixlen):k]) #get the prefix of 'encoded' activities\n",
    "      y = [0] *(mappingsize)\n",
    "      y[int(log[i][k])-1] = 1\n",
    "      Y.append(y)        \n",
    "  X = keras.preprocessing.sequence.pad_sequences(X, maxlen=prefixlen, padding='pre')\n",
    "  X = number_to_one_hot_X(X, mappingsize)\n",
    "  return(np.array(X), np.array(Y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yq2e1YsAkZiB"
   },
   "outputs": [],
   "source": [
    "def get_model(maxlen, num_chars, bidirec, n_layers, lstmsize, dropout, l1, l2):\n",
    "  model = Sequential()\n",
    "  model.add(Input(shape=(maxlen, num_chars))) #If you don't use an embedding layer input should be one-hot-encoded\n",
    "  if bidirec == False:   \n",
    "    model.add(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=(n_layers != 1),kernel_regularizer=regularizers.l1_l2(l1,l2),\n",
    "                   recurrent_regularizer=regularizers.l1_l2(l1,l2),input_shape=(maxlen, num_chars)))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(1, n_layers):\n",
    "      return_sequences = (i+1 != n_layers)\n",
    "      model.add(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=return_sequences,\n",
    "                     kernel_regularizer=regularizers.l1_l2(l1,l2),recurrent_regularizer=regularizers.l1_l2(l1,l2)))\n",
    "      model.add(Dropout(dropout))\n",
    "  else:\n",
    "    model.add(Bidirectional(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=(n_layers != 1),kernel_regularizer=regularizers.l1_l2(l1,l2),\n",
    "                   recurrent_regularizer=regularizers.l1_l2(l1,l2),input_shape=(maxlen, num_chars))))\n",
    "    model.add(Dropout(dropout))\n",
    "    for i in range(1, n_layers):\n",
    "      return_sequences = (i+1 != n_layers)\n",
    "      model.add(Bidirectional(LSTM(lstmsize,kernel_initializer='glorot_uniform',return_sequences=return_sequences,\n",
    "                     kernel_regularizer=regularizers.l1_l2(l1,l2),recurrent_regularizer=regularizers.l1_l2(l1,l2))))\n",
    "      model.add(Dropout(dropout))\n",
    "  model.add(Dense(num_chars, kernel_initializer='glorot_uniform',activation='softmax'))\n",
    "  opt = Adam(learning_rate=0.005)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics='accuracy')\n",
    "  return model\n",
    "\n",
    "\n",
    "def do_one_experiment(X_train, y_train, X_test, y_test,batch_size, maxlen, num_chars, bidirec, n_layers, lstmsize, dropout, l1, l2):\n",
    "  model = get_model(maxlen, num_chars, bidirec, n_layers, lstmsize, dropout, l1, l2)\n",
    "  model.summary()\n",
    "  early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
    "  lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "  #train_model\n",
    "  history = model.fit(X_train, y_train, validation_split=0.2, callbacks=[early_stopping, lr_reducer], batch_size=batch_size, epochs=600, verbose=2)\n",
    "  #test model on test set\n",
    "  loss, accuracy = model.evaluate(X_test, y_test)\n",
    "  return model, loss, accuracy\n",
    "\n",
    "def do_grid_search(df, modelname, full_prefix, opt_prefixlen, training_log_location,location_mapping_activity_to_number, modelfilename):\n",
    "  #add df just a way to continue training in seperate runs, keeps all results\n",
    "\n",
    "  full_log = remove_nan(import_log(training_log_location)) #import full event log\n",
    "\n",
    "  #if we want to use the full prefix each time or not\n",
    "  if full_prefix == True:\n",
    "    prefixlen=len(max(full_log,key=len)) -1\n",
    "    print(\"prefix length:\", prefixlen)\n",
    "  else:\n",
    "    prefixlen=opt_prefixlen\n",
    "\n",
    "  #reload mapping\n",
    "  mappingfilename = location_mapping_activity_to_number\n",
    "  with open(mappingfilename) as f:\n",
    "    mapping = json.loads(f.read())\n",
    "  #split into train and test traces, the log is randomly played out, so no need to mix\n",
    "  train_log = full_log[0: int(len(full_log) * 0.8)]\n",
    "  test_log = full_log[int(len(full_log) * 0.8):-1]\n",
    "\n",
    "  #convert into prefix + target, both one_hot_encoded\n",
    "  X_train, y_train = create_XY_prefix(train_log, len(mapping), prefixlen)\n",
    "  X_test, y_test = create_XY_prefix(test_log, len(mapping), prefixlen)\n",
    "  \n",
    " \n",
    "\n",
    "  grid_is_bidirectional = [False, True]\n",
    "  grid_nr_layers = [1, 2]\n",
    "  grid_layer_size = [16, 32, 64]\n",
    "  grid_reg = [0.0, 0.0001, 0.001, 0.01]\n",
    "  grid_dropout = [0.0 ,0.2, 0.4, 0.6]\n",
    "\n",
    "  batch_size = 128\n",
    "\n",
    "  i = 0 #keep track of index in grid\n",
    "\n",
    "  for is_bidirectional in grid_is_bidirectional:\n",
    "    for nr_layers in grid_nr_layers:\n",
    "      for layer_size in grid_layer_size:\n",
    "        for reg in grid_reg:\n",
    "          for dropout in grid_dropout:\n",
    "            if (i in df.index) == True:\n",
    "              print(i, 'This setting already in dataframe:', is_bidirectional,nr_layers,layer_size,reg,dropout)\n",
    "            else:\n",
    "              model, loss, accuracy = do_one_experiment(X_train, y_train, X_test, y_test,batch_size, maxlen=prefixlen, num_chars=len(mapping), bidirec=is_bidirectional, n_layers=nr_layers, lstmsize=layer_size, dropout=dropout, l1=reg, l2=reg)\n",
    "              new_row = {'Bidirectional':is_bidirectional,'Nr_layers':nr_layers,'Layer_size':layer_size, 'Regularization':reg,'Dropout':dropout,'Loss':loss,'Accuracy':accuracy}\n",
    "              df = df.append(new_row, ignore_index=True)\n",
    "              print(new_row)\n",
    "              df.to_csv(path + 'Results_grid.csv')\n",
    "              model.save(modelfilename)\n",
    "            i = i+1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "name": "Hyper_Model1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
